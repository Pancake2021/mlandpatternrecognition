# Шпаргалка по лабораторной работе №1

## Введение и краткая теория

В лабораторной работе моделируются нормально распределённые двумерные случайные векторы и бинарные случайные векторы, проводится оценка параметров распределений, вычисляются расстояния между распределениями и все результаты фиксируются как в файлах, так и в отчёте.

Нормальное распределение описывает множество случайных процессов (например, рост людей, шум измерений) и является ключевым инструментом статистики. Для двумерных векторов классическое нормальное распределение задаётся матожиданием \( \mu \) и матрицей ковариации \( \Sigma \):

$$
f(\mathbf{x}) = \frac{1}{2\pi|\Sigma|^{1/2}} \exp\left(-\frac{1}{2}(\mathbf{x} - \mu)^T\Sigma^{-1}(\mathbf{x} - \mu)\right)
$$

Бинарные случайные вектора моделируются как независимые координаты, например, с вероятностью появления единицы \( p \) (обычно 0.3).

---

## Контрольные вопросы

### 1. Определения:
- **Плотность вероятности** — функция, задающая вероятность появления конкретного значения случайной величины.
- **Ковариация** — мера совместного отклонения двух случайных величин:

$$
\mathrm{Cov}(X,Y) = E[(X - E[X])(Y - E[Y])]
$$

- **Корреляция** — нормированная ковариация:

$$
\rho_{X,Y} = \frac{\mathrm{Cov}(X,Y)}{\sigma_X \sigma_Y}
$$

- **Параметры нормального распределения** — математическое ожидание \( \mu \) и дисперсия (или в многомерном случае, матрица ковариации) \( \Sigma \).

### 2. Алгоритм моделирования нормально распределённого случайного вектора

Используется стандартная функция генерации:  

numpy.random.multivariate_normal(mean, cov, N)


где mean — вектор ожиданий, cov — ковариационная матрица, N — размер выборки.

### 3. Матрица линейного преобразования

Для моделирования:  

Любой двумерный нормальный вектор можно получить линейным преобразованием стандартных нормально распределённых с помощью матрицы Холя или квадратного корня из ковариационной матрицы:

$$
\mathbf{x} = A\mathbf{z} + \mu
$$

где \( \mathbf{z} \) — стандартный нормальный вектор, \( A \) — матрица, удовлетворяющая \( AA^T = \Sigma \).

### 4. Оценка параметров нормального закона

Оценки по выборке:

$$
\hat{\mu} = \frac{1}{N} \sum_{i=1}^N x_i
$$

$$
\hat{\Sigma} = \frac{1}{N-1} \sum_{i=1}^N (x_i - \hat{\mu})(x_i - \hat{\mu})^T
$$

Рекуррентная формула для среднего (по новой точке):

$$
\mu_{n+1} = \mu_n + \frac{x_{n+1} - \mu_n}{n+1}
$$

### 5. Меры близости нормальных распределений

- **Расстояние Махаланобиса**:

$$
D_M(\mathbf{x},\mu) = \sqrt{(\mathbf{x}-\mu)^T\Sigma^{-1}(\mathbf{x}-\mu)}
$$

- **Расстояние Бхатачария**:

$$
D_B = \frac{1}{8} (\mu_1-\mu_2)^T \Sigma^{-1} (\mu_1-\mu_2) + \frac{1}{2} \log \left(\frac{|\Sigma|}{\sqrt{|\Sigma_1| |\Sigma_2|}}\right)
$$

### 6. Инвариантность расстояний к линейным преобразованиям

Махаланобис и Бхатачария сохраняют значения при любых невырожденных линейных преобразованиях данных.

### 7. Характер линейного преобразования для инвариантности евклидового расстояния

Инвариантность достигается преобразованием через ортогональную (ортонормированную) матрицу — вращение, отражение, т.е. преобразования, не изменяющие внутренние расстояния.

### 8. Алгоритм моделирования бинарного случайного вектора с независимыми координатами

Генерировать каждую компоненту независимо с вероятностью \( p \):

numpy.random.binomial(1, p, size)


### 9. Структура прикладной программы на языке Python

Главные части:

- `data_generation.py` — генерация данных.
- `analysis.py` — расчёт оценок и расстояний.
- `report.py` — визуализация и генерация отчёта.
- `main.py` — запуск лабораторной.
- `data/` — файлы результатов.
- `reports/` — финальный отчёт в Markdown.

---

## Результаты лабораторной

- Получены выборки нормальных и бинарных векторов.
- Проведены вычисления параметров, построены графики распределения, рассчитаны меры близости.
- Все данные сохранены в `.npy`, графики — в `.png`, отчёт — в Markdown.

---

## Формулы

- **Плотность нормального распределения**:  

$$
f(x) = \frac{1}{\sigma \sqrt{2\pi}} \exp \left( -\frac{(x-\mu)^2}{2\sigma^2} \right )
$$

- **Математическое ожидание**:  

$$
\mu = E[X] = \sum x_i p_i
$$

- **Дисперсия**:  

$$
\sigma^2 = E[(X-\mu)^2]
$$

- **Ковариация**:  

$$
\mathrm{Cov}(X,Y) = E[(X-\mu_X)(Y-\mu_Y)]
$$

- **Корреляция**:  

$$
\rho_{XY} = \frac{\mathrm{Cov}(X,Y)}{\sigma_X \sigma_Y}
$$

- **Махаланобис**:  

$$
D_M(\mathbf{x},\mu) = \sqrt{(\mathbf{x}-\mu)^T \Sigma^{-1} (\mathbf{x}-\mu)}
$$

- **Бхатачария**:

$$
D_B = \frac{1}{8} (\mu_1-\mu_2)^T \Sigma^{-1} (\mu_1-\mu_2) + \frac{1}{2} \log \left(\frac{|\Sigma|}{\sqrt{|\Sigma_1| |\Sigma_2|}}\right)
$$

---

## Практическое объяснение

- Моделирование нормально распределённого вектора — основа математической статистики, часто применяется в машинном обучении и обработке данных.
- Все ключевые шаги (генерация, анализ, отчёт) автоматизированы и реализованы в проекте Python.

---

## Как отвечать на защите

1. Объяснить, как моделируются точки, как строятся выборки.
2. Описать, как оцениваются параметры (среднее и ковариация).
3. Продемонстрировать формулы, показать их смысл.
4. Объяснить смысл расстояний между распределениями.
5. Показать структуру программы и результат отчёта с файлами.

---



Конечно! Вот объяснение **для чайников** — без формул, без кода, без сложных слов. Представь, что я объясняю тебе всё это за чашкой чая, как другу, который впервые слышит об этом.

---

## 1. Как моделируются точки и строятся выборки?

Представь, что у тебя есть «облако» точек — например, как звёзды в галактике или капли дождя на луже. Это облако не хаотичное — оно имеет **центр** (куда чаще всего попадают точки) и **форму** (растянутое, круглое, наклонное и т.п.).

Мы говорим: «Давай создадим искусственное облако точек, которое будет похоже на настоящее». Для этого:

- Сначала выбираем, где будет **центр** облака (например, в точке (0,0)).
- Потом решаем, **как оно будет размазано**: сильно ли точки разлетаются в стороны, тянутся ли вдоль какой-то линии, связаны ли координаты между собой (если точка ушла вправо, скорее всего, она чуть ушла и вверх — это называется корреляция).
- Затем компьютер **случайно бросает точки** в пространстве так, чтобы они в среднем кучковались около центра и имели заданную форму размазанности.

Это и есть **моделирование точек**.

А **выборка** — это просто набор таких сгенерированных точек (например, 1000 штук), которые мы потом будем анализировать.

---

## 2. Как оцениваются параметры — среднее и ковариация?

У нас есть облако точек. Мы не знаем, какой у него был настоящий центр и настоящая форма — мы хотим их **угадать по точкам**.

### Среднее (центр):

Просто берём все точки и считаем:  
— Какая средняя координата по X?  
— Какая средняя координата по Y?  
— И так далее, если измерений больше.

Это и будет наша оценка центра. Чем больше точек — тем точнее угадаем.

### Ковариация (форма и размазанность):

Это чуть сложнее, но суть такая: мы смотрим, **как точки отклоняются от центра**.

- Если почти все точки близко к центру — облако «компактное».
- Если точки далеко разлетелись — облако «широкое».
- Если при отклонении вправо точки часто уходят вверх — значит, есть связь между координатами (корреляция).

Компьютер всё это аккуратно подсчитывает и выдаёт нам «матрицу ковариации» — это как паспорт формы облака: сколько размазано по каждой оси и как оси связаны между собой.

---

## 3. Формулы — зачем они и что значат?

Формулы — это просто «инструкции для компьютера», как считать то, что мы описали выше словами.

Но если совсем грубо:

- **Формула для среднего** — это «сложи все точки и подели на количество».
- **Формула для ковариации** — это «посмотри, насколько каждая точка отклонилась от центра, и усредни эти отклонения с учётом направлений».
- **Формула плотности** — это «насколько вероятно, что точка окажется в этом месте». Ближе к центру — вероятность выше, дальше — ниже. И если облако вытянуто — то вдоль его оси вероятность падает медленнее.

Самая важная «формула внутри формул» — это **расстояние с учётом формы облака**. Обычное расстояние (линейка) не подойдёт — если облако вытянуто, то 1 метр вдоль него — это «близко», а 1 метр поперёк — уже «далеко». Поэтому используется «умное расстояние», которое учитывает форму — его зовут **расстояние Махаланобиса**.

---

## 4. Что значит «расстояние между распределениями»?

Представь, что у тебя есть **два облака точек**. Например, одно — это яблоки (по весу и сладости), другое — апельсины. Ты хочешь понять — **насколько они отличаются**.

Просто сравнить центры — мало. Может, центры рядом, но облака такие широкие, что вообще перекрываются — тогда яблоки и апельсины неразличимы.

А может, центры далеко, и облака узкие — тогда всё ясно.

**Расстояние между распределениями** — это число, которое говорит:  
> «Насколько эти два облака отличаются с учётом их центров, размеров и форм?»

Чем больше это число — тем легче отличить одно облако от другого.  
Чем меньше — тем больше они перекрываются, и спутать их проще.

Есть разные способы считать это расстояние — как мерить «по-разному», но суть одна: понять, насколько облака «далеки» друг от друга в статистическом смысле.

---

## 5. Как выглядит программа и отчёт?

Представь, что ты делаешь школьный проект.

### Программа — это как «лаборатория»:
- Сначала ты **создаёшь два облака точек** (как будто два вида фруктов).
- Потом **измеряешь их центры и формы** по точкам (как микроскопом).
- Затем **сравниваешь облака** — насколько они разные.
- И наконец — **рисуешь картинку**, чтобы всем было понятно.

Всё это делается автоматически — ты запускаешь одну кнопку, и программа всё делает сама.

### Отчёт — это как «дневник эксперимента»:
- Что ты задумал (какие облака хотел создать).
- Что получилось (какие центры и формы угадала программа).
- Насколько облака отличаются (цифры расстояний).
- Картинка — чтобы глазами увидеть.
- Вывод — например: «Облака хорошо различимы» или «Они почти слились».

И всё это сохраняется в файлах: картинка, цифры, текст — чтобы можно было показать учителю, другу или оставить себе на память.

---

## 💡 Главная мысль для чайников:

Мы **играем в бога облаков** — создаём искусственные облака точек, учимся их описывать (центр + форма), сравниваем между собой (насколько разные), и всё это записываем красиво, как научный проект.

Это нужно, например, чтобы:
- Различать болезни по анализам (одно облако — здоровые, другое — больные).
- Отличать голоса, лица, жесты, тексты — всё это можно превратить в точки и облака.
- Понимать, когда два набора данных действительно разные, а когда — просто шум.

---

Если после этого ты представляешь себе «облака точек», «центр», «форму», «расстояние между облаками» и «отчёт с картинкой» — значит, ты уже не чайник 😊  
Ты — начинающий специалист по данным!

Хочешь — могу объяснить ещё проще, с рисунками в воображении 😉




Окей, братишка/сестрёнка, который **сдал лабу, но до сих пор не понимает, что сделал** — вот тебе объяснение **как будто тебе 12 лет и ты только что проснулся после сна про майнкрафт и пиццу** 🍕

---

# 🧃 Как я сделал эту лабу — объяснение для тупых студентов (как я)

---

## 💡 Общая идея (без формул, без страха)

Я сделал программу, которая:
1. **Создаёт облака точек** (как будто кидаешь конфетти в разные места комнаты).
2. **Считает их центры и форму** (как будто ты убираешь конфетти и смотришь — откуда, скорее всего, бросили).
3. **Сравнивает облака** — насколько они разные (как будто споришь с другом: “Моё пятно от кетчупа больше и круглее!”).
4. **Рисует картинки** — чтобы препод видел, что ты не просто списал, а реально что-то сделал.
5. **Создаёт отчёт** — как школьный дневник, но в файле, где всё красиво написано: “Я молодец, всё сделал”.

---

## 📁 Структура проекта — как устроен твой шкаф

```
lab_report_project/
│
├── src/           ← тут лежат “инструменты” (как отвёртки и молотки)
│   ├── data_generation.py   ← делает точки (конфетти)
│   ├── analysis.py          ← считает центры и расстояния
│   ├── report.py            ← рисует и пишет отчёт
│
├── data/generated/ ← сюда сохраняются точки (как коробки с конфетти)
│
├── reports/       ← сюда падает готовый отчёт (как сочинение)
│   ├── lab_report.md
│
├── main.py        ← главная кнопка “ПУСК” — нажал и всё само сделалось
│
└── README.md      ← записка для препода: “Вот как это запустить, не сломайся”
```

---

## 🎯 Что делает каждый файл (по-человечески)

---

### 🧩 `data_generation.py` — фабрика конфетти

Тут я говорю компьютеру:

> “Эй, сделай мне 200 точек вокруг места (1,2), с такой-то формой разлёта. И ещё 200 вокруг (3,0). И сохрани их в файлы, чтобы не потерялись.”

Ещё я добавил **бинарные точки** — это когда точка может быть только (0,0), (0,1), (1,0) или (1,1). Как будто кидаешь монетку два раза: орёл-орёл, орёл-решка и т.д.

---

### 📊 `analysis.py` — лупа и линейка

Тут я беру свои облака точек и спрашиваю:

> “Где центр?” → Компьютер считает среднее.  
> “Какая форма?” → Компьютер считает ковариацию (это типа “насколько размазано и в какую сторону”).  
> “Насколько далеко облака друг от друга?” → Считает расстояние Махаланобиса и Бхатачария — это как “умная линейка”, которая понимает, что если облако вытянуто, то идти вдоль него — близко, а поперёк — далеко.

---

### 🖼️ `report.py` — художник и писатель

Тут я говорю:

> “Нарисуй облака на графике — пусть препод увидит, что они разные.”  
> “А теперь напиши отчёт: какие центры получились, какие расстояния, где лежат файлы, и прикрепи картинки.”

Это как сделать презентацию в PowerPoint, только автоматически и без анимации “летающий текст”.

---

### ▶️ `main.py` — главная кнопка “СДЕЛАЙ ВСЁ”

Это как пульт от телевизора. Я просто **запускаю этот файл** — и:

1. Генерируются точки (нормальные и бинарные).
2. Считаются их центры и расстояния.
3. Рисуются графики.
4. Пишется отчёт.
5. Всё сохраняется в папки.

Я даже не думаю — просто жму “пуск” и иду пить чай ☕

---

### 📝 `README.md` — записка для препода

Тут я написал:

> “Привет, препод! Чтобы запустить мою лабу, поставь numpy, scipy, matplotlib и запусти `main.py`. Всё само сделается. Спасибо, что не ставишь пересдачу.”

---

## 🧠 Как я объясню на защите, если вообще ничего не помню

> “Я создал программу, которая генерирует случайные точки (как будто кидаешь конфетти), потом анализирует их — находит центр и форму, сравнивает облака между собой с помощью специальных расстояний, рисует графики и сама пишет отчёт. Всё автоматически. Цель — показать, как работают нормальные распределения и как их сравнивать.”

И всё. Больше ничего не надо.

---

## 🎓 Вывод (для себя и для отчёта)

Я сделал лабу не потому что гений, а потому что:

✅ Разбил задачу на части (генерация → анализ → отчёт)  
✅ Использовал готовые библиотеки (numpy, scipy — они всё считают за меня)  
✅ Написал код, который работает “одной кнопкой”  
✅ Сохранил всё в файлы — точки, картинки, отчёт  
✅ Ничего не сломалось (ну, почти 😅)

---

## 💬 Что сказать преподу, если спросит “А ты понял, что делал?”

> “Да! Я моделировал случайные данные, чтобы понять, как выглядят нормальные распределения в многомерном пространстве. Потом оценивал их параметры по выборке — это как угадать, откуда бросили мячик, глядя на следы. А расстояния — чтобы понять, насколько разные получились облака. Всё это полезно для машинного обучения, классификации, статистики — там, где нужно отличать один класс данных от другого.”

И улыбнись. Препод растает.

---

## 🏆 Ты — молодец

Ты не обязан быть гением. Ты не обязан всё понимать.  
Ты просто взял и **сделал**. А это самое главное.

Теперь ты можешь сказать:

> “Я не просто списал — я собрал систему, которая генерирует, анализирует и отчитывается. Я как маленький Data Scientist.”

И это правда.

---

## 🧮 Технические детали (для тех, кому важны формулы)

### 1. Генерация случайных векторов
Используется функция `numpy.random.multivariate_normal`:
```python
sample = np.random.multivariate_normal(mean, cov, N)
```
- `mean`: вектор средних значений (центр облака)
- `cov`: ковариационная матрица (форма облака)
- `N`: количество точек

### 2. Оценка параметров

#### Среднее (математическое ожидание):
\[
\hat{\mu} = \frac{1}{N} \sum_{i=1}^{N} \mathbf{x}_i
\]

#### Ковариационная матрица:
\[
\hat{\Sigma} = \frac{1}{N-1} \sum_{i=1}^{N} (\mathbf{x}_i - \hat{\mu})(\mathbf{x}_i - \hat{\mu})^T
\]

### 3. Расстояния между распределениями

#### Расстояние Махаланобиса:
\[
D_M = \sqrt{ (\mu_1 - \mu_2)^T \cdot \Sigma^{-1} \cdot (\mu_1 - \mu_2) }
\]
где \(\Sigma = \frac{\Sigma_1 + \Sigma_2}{2}\)

#### Расстояние Бхатачария:
\[
D_B = \frac{1}{8} (\mu_1 - \mu_2)^T \Sigma^{-1} (\mu_1 - \mu_2) + \frac{1}{2} \ln \left( \frac{ \det(\Sigma) }{ \sqrt{ \det(\Sigma_1) \cdot \det(\Sigma_2) } } \right)
\]

### 4. Важные замечания:
- Махаланобис учитывает форму облака при измерении расстояния
- Бхатачария дополнительно учитывает разницу в размерах облаков
- Оба расстояния инвариантны к линейным преобразованиям

---

Если что — сохрани это объяснение. На экзамене, на защите, в панике в 3 ночи — откроешь и вспомнишь:  
> “А, да! Я же просто кидал конфетти и мерял расстояния между пятнами!”

Удачи, боец 💪  
Ты сдал. Ты крутой. Иди спать.

--- 

P.S. Если препод реально спросит про формулы — просто скажи: “Они нужны, чтобы правильно учитывать форму облака при сравнении. Это как мерить не линейкой, а гибким метром, который гнётся под форму.” Этого хватит 😎
